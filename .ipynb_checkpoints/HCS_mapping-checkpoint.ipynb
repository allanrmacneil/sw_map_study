{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to compute longitude offsets between 2.5 Rs and 1 AU HCS crossings per the methodology of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:39.465762Z",
     "start_time": "2021-08-04T11:37:25.372439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do our imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "from scipy.special import expit\n",
    "\n",
    "import datetime\n",
    "\n",
    "import heliopy.data.omni as omni\n",
    "\n",
    "from sunpy.coordinates import sun,frames,ephemeris\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import astropy.constants as const\n",
    "from sunpy.sun import constants as con\n",
    "\n",
    "from DL_GONG import DL_GONG\n",
    "import DL_Drive\n",
    "import sunpy \n",
    "import pfsspy\n",
    "from pfsspy import tracing\n",
    "from pfsspy import coords\n",
    "from spin_angle import re_angle,re_angle_deg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:39.485203Z",
     "start_time": "2021-08-04T11:37:39.468835Z"
    }
   },
   "outputs": [],
   "source": [
    "# these study start and end times are based on GONG availability\n",
    "startdate = datetime.datetime(2006,9,20,0)\n",
    "enddate = datetime.datetime(2021,1,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Omni data HCS identification #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:45:03.147621Z",
     "start_time": "2021-08-04T11:44:43.514485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qKdjrF-VQ9wy-Xp1r1X-8OfgSmQV4zWk\n",
      "To: /home/allan/Code/SW_Mapping_Project/sw_map_study/omni_2006_21.hdf\n",
      "60.6MB [00:13, 4.66MB/s]\n",
      "/home/allan/Code/SW_Mapping_Project/sw_map_study/spin_angle.py:13: RuntimeWarning: invalid value encountered in less\n",
      "  under = (ang_in < low)\n",
      "/home/allan/Code/SW_Mapping_Project/sw_map_study/spin_angle.py:14: RuntimeWarning: invalid value encountered in greater\n",
      "  over =  (ang_in > high)\n",
      "<ipython-input-3-a50b212c7c40>:29: RuntimeWarning: invalid value encountered in greater\n",
      "  dat['park_pol'].iloc[abs(dat.dParker.values) > 90 ] = -1\n",
      "/home/allan/miniconda3/envs/heliomap/lib/python3.8/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Code to read in or download the hourly OMNI data. \n",
    "\n",
    "process = False # are we going to have to download new files or do we already have them?\n",
    "\n",
    "if process:\n",
    "    dat = omni.h0_mrg1hr(startdate,enddate).to_dataframe()\n",
    "    eartheph = ephemeris.get_earth(time=dat.index)\n",
    "    dat['CarLat']= eartheph.lat.value\n",
    "    dat['r'] = eartheph.radius.value\n",
    "    \n",
    "    # these ephemeris calls are s l o w !\n",
    "    dat['CarLon']=sun.L0(time=dat.index).value\n",
    "    dat['CR'] = np.floor(sun.carrington_rotation_number(dat.index)).astype(int)\n",
    "    dat.to_hdf('omni_2006_21.hdf',key='df',mode='w')\n",
    "else:\n",
    "    fname = 'omni_2006_21.hdf'\n",
    "    DL_Drive.DL_Drive(fname) # download pre-run file from my drive\n",
    "    dat = pd.read_hdf(fname) # open it\n",
    "\n",
    "# some duplicate times exist in the omni data so chuck those\n",
    "dat = dat.loc[~dat.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "dat['B_angle'] = np.arctan2( dat.BX_GSE,dat.BY_GSE )*180./np.pi #-45 is + Br parker spiral, 135 is -ve Br.\n",
    "dat['Parker_angle'] = np.arctan2( (con.get('sidereal rotation rate')*const.au),dat.V.values*u.km*u.rad/u.s).to(u.deg).value\n",
    "dat['dParker'] = dat.B_angle +dat.Parker_angle # angle from nominal Parker spiral angle of 45 degrees\n",
    "dat['dParker'] = re_angle_deg(dat.dParker.values,-180,180) # make this angle go from - to + 180\n",
    "dat['park_pol'] = np.ones(dat.B_angle.size) # define the polarity relative to the Parker spiral\n",
    "dat['park_pol'].iloc[abs(dat.dParker.values) > 90 ] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:45.384343Z",
     "start_time": "2021-08-04T11:37:41.406752Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-9caf7901ca34>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  HCS_Dates = pd.read_csv('sblist.txt',names=cols,sep = '\\s+|,|\\t')\n"
     ]
    }
   ],
   "source": [
    "# We're going to use the HCS list from leif svalgaard http://wso.stanford.edu/SB/SB.Svalgaard.html. This only gives crossing dates\n",
    "# I will use these as a starting point to check for a precise crossing time within\n",
    "\n",
    "# code to read the txt file\n",
    "cols= ['pre','post','year','month','day','pretime','posttime']\n",
    "HCS_Dates = pd.read_csv('sblist.txt',names=cols,sep = '\\s+|,|\\t')\n",
    "datedf = HCS_Dates[['year','month','day']]\n",
    "\n",
    "# create the dataframe\n",
    "sec_df =pd.to_datetime(datedf)\n",
    "sec_df = pd.DataFrame({'pretime':HCS_Dates['pretime'].values,'posttime':HCS_Dates['posttime'].values,'flip':HCS_Dates['pre'].values},index=sec_df)\n",
    "\n",
    "# define carrington longitude and carrington number from the time using sunpy\n",
    "sec_df['CarLon']=sun.L0(time=sec_df.index).value\n",
    "sec_df['CR'] = np.floor(sun.carrington_rotation_number(sec_df.index)).astype(int)\n",
    "\n",
    "# reduce the dataframe only to times we have data\n",
    "sec_df = sec_df.iloc[sec_df.index > startdate]\n",
    "\n",
    "# add a variable that defines the sign change of the sector crossing\n",
    "sec_df['flip_int']=np.zeros(sec_df.flip.size).astype(int)\n",
    "sec_df['flip_int'].iloc[sec_df.flip.values=='+']=1\n",
    "sec_df['flip_int'].iloc[sec_df.flip.values=='-']=-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:49.218356Z",
     "start_time": "2021-08-04T11:37:45.387151Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a v for each crossing using the omni data in a 24 hour centred window\n",
    "\n",
    "for t in sec_df.index: # do a loop\n",
    "    ind = abs(t-dat.index) < pd.Timedelta(days=.5) # pick index in window\n",
    "    sec_df.loc[t,'V'] = dat.V[ind].mean() # find mean v\n",
    "    sec_df.loc[t,'n_in_V'] = np.sum(ind) # number of samples in v\n",
    "    sec_df.loc[t,'n'] = dat.N[ind].mean() \n",
    "    sec_df.loc[t,'B'] = dat.ABS_B[ind].mean() \n",
    "    sec_df.loc[t,'Br'] = abs(dat.BX_GSE[ind]).mean()\n",
    "    ind_cen = np.argmin(abs(t - dat.index))\n",
    "    \n",
    "    # also calcualte velocity gradient although I don't use it any more\n",
    "    ind_plus = np.arange(ind_cen,ind_cen+12) # this works since it's 12 hour\n",
    "    ind_minus = np.arange(ind_cen-12,ind_cen)\n",
    "    sec_df.loc[t,'Vgrad'] = dat.V[ind_plus].mean() - dat.V[ind_minus].mean() # so for increasing V, this will be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:51.399880Z",
     "start_time": "2021-08-04T11:37:49.221230Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-01167935513c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use a logistic regression approach to get a most probable HCS location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# use a logistic regression approach to get a most probable HCS location\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def flip_finder(df, dat,rm_perp=False,rm_perp_limit=30,nday = 2):\n",
    "    '''Code to find the best fitting current sheet crossing location/time\n",
    "    df is my sector data\n",
    "    dat is my omni data\n",
    "    rm_perp: optionally discount samples with dParker angle within rm_perp_limit degrees of 90\n",
    "    nday: number of days either side of the date in df to look for the crossing within\n",
    "    '''\n",
    "    bad6 = np.zeros(df.shape[0]) # create an empty array to flag if something fails\n",
    "    \n",
    "    i=0 # bad loop design\n",
    "    for index,entry in df.iterrows(): # iterate over each crossing in df\n",
    "        fliptype = entry.flip_int\n",
    "        day_offset = 6/24. # an offset to accoutnt for when the flip is likely to happen in the day. if they give the date it isn't going to be at midnight on average\n",
    "        \n",
    "        # pull out relevant part of the omni data:\n",
    "        subdat_index = ( dat.index.floor('D')>=  index.floor('D')-pd.Timedelta(days=nday -day_offset) )  & ( dat.index.floor('D') <=  index.floor('D')+pd.Timedelta(days=nday+day_offset)  )\n",
    "        subdat = dat.iloc[subdat_index]\n",
    "        \n",
    "        if subdat_index.sum()>1: # only continue if there are > 1 omni data points available\n",
    "            t = (subdat.index-subdat.index[0]).total_seconds().values # this is the time relative to the window\n",
    "            y= subdat.park_pol.values # we'll be fitting the polarity as a logistic function\n",
    "            tmax= np.max(t) \n",
    "            t=t/tmax # normalise time between 0 and 1, which is needed by the regression\n",
    "            \n",
    "            # do the removal of samples that are too close to ambiguous polarity\n",
    "            if rm_perp:\n",
    "                ydrop = ( abs(subdat.dParker.values) > 90-rm_perp_limit) & (abs(subdat.dParker.values) < 90 + rm_perp_limit)\n",
    "                y=y[~ydrop]\n",
    "                t=t[~ydrop]\n",
    "    \n",
    "            if np.any(y==1) and np.any(y==-1): # so if it's not all ones or zeros (i.e., there's an actual change in polarity)\n",
    "                clf = LogisticRegression(solver='liblinear',C=10000).fit(t.reshape(-1, 1) , y) # do the regression fit\n",
    "                \n",
    "                # extract and save properties of the regression fit\n",
    "                df.loc[index,'coef'] =clf.coef_[0][0] \n",
    "                df.loc[index,'intercept']=clf.intercept_[0]\n",
    "                df.loc[index,'score'] = clf.score(t.reshape(-1,1),y)\n",
    "                \n",
    "                df.loc[index,'t_c'] = (-clf.intercept_[0]/clf.coef_[0][0]) # crossing time in seconds\n",
    "                df.loc[index,'t_cross'] = pd.Timedelta(seconds=(-clf.intercept_[0]/clf.coef_[0][0])*tmax) + subdat.index[0] # crossing time in datetime, from adding the first time index back in\n",
    "                df.loc[index,'frac_pos'] = np.sum(y>0)/(np.sum(y!=0)) # fraction of samples which are positive polarity\n",
    "                df.loc[index,'frac_valid'] = np.sum(y!=0)/(24*2*nday) # fraction which are valid (out of possible total)\n",
    "                fitflip = expit(clf.coef_[0][0]*t + clf.intercept_[0])[-1]-expit(clf.coef_[0][0]*t + clf.intercept_[0])[0] > 0 # boolean to indicate which direction the polarity flips\n",
    "                if fitflip:\n",
    "                    df.loc[index,'fit_flip'] = -1 # and record that as -1 or +1\n",
    "                else:\n",
    "                    df.loc[index,'fit_flip'] = 1\n",
    "            else:\n",
    "                bad6[i]=True # if there's not then set this flag\n",
    "        i+=1\n",
    "    df['hrs_offset'] = (df.t_cross-(df.index + pd.Timedelta(day_offset) )).values/ np.timedelta64(1, 'h') # compute how far the crossing time is from the original date\n",
    "    \n",
    "    \n",
    "    # also figure out if the result is good or bad\n",
    "    P = 27.2752612 # solar rotation period in days as seen from Earth. We use synodic period here\n",
    "    lon_offset = 360*(df.hrs_offset/24)/P\n",
    "\n",
    "    # conditions for throwing away data:\n",
    "    bad1 = abs(df.hrs_offset)> 48 # can't predict crossing time outside of the window\n",
    "    bad2 = df.score<.8 # limit based on how good the regression is\n",
    "    bad3 = (df.frac_pos > .9) | (df.frac_pos < .1) # can't predict too much of one polarity\n",
    "    bad4 = df.frac_valid<.5 # need at least 50% of data filled in\n",
    "    bad5 = np.isnan(df.frac_valid.values.astype(float)) # also if the valid parameter is NaN\n",
    "    rem = bad1 | bad2 | bad3 | bad4 | bad5 | bad6  # combine them all with inclusive or \n",
    "    lon_offset[rem]=np.nan\n",
    "\n",
    "    df['lon_cross'] = sun.L0(time=sec_df.t_cross).value # crossing longitude from epehemeris to be safe\n",
    "    df['lon_cross'][rem] = np.nan\n",
    "    return(df,rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:56.702938Z",
     "start_time": "2021-08-04T11:37:51.402685Z"
    }
   },
   "outputs": [],
   "source": [
    "# call the flip finder code with my options\n",
    "\n",
    "rm_perp=True\n",
    "rm_perp_limit = 25   \n",
    "nday = 3\n",
    "\n",
    "sec_df,rem = flip_finder(sec_df,dat,rm_perp=rm_perp,rm_perp_limit=rm_perp_limit,nday=nday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:56.908283Z",
     "start_time": "2021-08-04T11:37:56.708396Z"
    }
   },
   "outputs": [],
   "source": [
    "# just having a look at the crossing times and making sure my removal is working\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(131)\n",
    "plt.hist(sec_df.t_c,range=[-1,2],bins=20)\n",
    "plt.hist(sec_df.t_c.values[~rem],range=[-1,2],bins=20,histtype='step')\n",
    "plt.xlabel('t_c')\n",
    "plt.subplot(132)\n",
    "plt.hist(sec_df.hrs_offset.values,range=[-48,48],bins=20,density=True)\n",
    "plt.hist(sec_df.hrs_offset.values[~rem],range=[-48,48],bins=20,density=True,histtype='step')\n",
    "plt.xlabel('Hours offset')\n",
    "plt.subplot(133)\n",
    "plt.hist(sec_df.score.values,bins=20,cumulative=True,density=True)\n",
    "plt.xlabel('Regression Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:58.286314Z",
     "start_time": "2021-08-04T11:37:56.912250Z"
    }
   },
   "outputs": [],
   "source": [
    "# An example plot of how the regression works for some low scores\n",
    "from numpy.random import choice \n",
    "\n",
    "sec_df_sub = sec_df.iloc[sec_df.score.values<0.8]\n",
    "\n",
    "indexes = np.arange(0,sec_df_sub.index.size).astype(int)\n",
    "\n",
    "ind = choice(indexes,size=5)\n",
    "\n",
    "\n",
    "fig= plt.figure(figsize=(14,4))\n",
    "i=1\n",
    "for index,entry in sec_df_sub.iloc[ind].iterrows():\n",
    "    fliptype = entry.flip_int\n",
    "    day_offset = 6/24. # an offset to accoutnt for when the flip is likely to happen in the day\n",
    "    subdat_index = ( dat.index.floor('D')>=  index.floor('D')-pd.Timedelta(days=nday -day_offset) )  & ( dat.index.floor('D') <=  index.floor('D')+pd.Timedelta(days=nday+day_offset)  )\n",
    "    subdat = dat.iloc[subdat_index]\n",
    "    t = (subdat.index-subdat.index[0]).total_seconds().values\n",
    "    tmax= np.max(t)\n",
    "    t=t/tmax\n",
    "\n",
    "    ax=plt.subplot(2,6,i)\n",
    "    y= subdat.park_pol\n",
    "    plt.plot(abs(subdat.dParker),'.',alpha=.5)\n",
    "    plt.axvline(entry.t_cross,color='k')\n",
    "    if i ==1: plt.ylabel('$\\psi_P$')\n",
    "    ax2=plt.subplot(2,6,i+6)\n",
    "    y= subdat.park_pol\n",
    "    ydrop = ( abs(subdat.dParker.values) > 90-rm_perp_limit) & (abs(subdat.dParker.values) < 90 + rm_perp_limit)\n",
    "    plt.plot(subdat.index,y,'.')\n",
    "    plt.plot(subdat.index[~ydrop],y[~ydrop]*.8,'.')\n",
    "    if i == 1: plt.ylabel('Polarity')\n",
    "    plt.axvline(entry.t_cross,color='k')\n",
    "    ax3=ax2.twinx()\n",
    "    plt.plot(subdat.index,expit(entry.coef*t+entry.intercept),color='r')\n",
    "    plt.title(f'{entry.score:.2f}')\n",
    "    plt.ylim(0,1)\n",
    "    i+=1\n",
    "fig.autofmt_xdate()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:59.277818Z",
     "start_time": "2021-08-04T11:37:58.288793Z"
    }
   },
   "outputs": [],
   "source": [
    "# choose some random crossings to plot, which have characteristically good or bad scores. I also print whether they're removed\n",
    "\n",
    "sec_df_sub = sec_df.iloc[sec_df.score.values>0.8]\n",
    "rem_sub = rem[sec_df.score.values>0.8]\n",
    "indexes = np.arange(0,sec_df_sub.index.size).astype(int)\n",
    "\n",
    "ind = choice(indexes,size=5)\n",
    "\n",
    "fig = plt.figure(figsize=(14,4))\n",
    "i=1\n",
    "for index,entry in sec_df_sub.iloc[ind].iterrows():\n",
    "    fliptype = entry.flip_int\n",
    "    day_offset = 6/24. # an offset to accoutnt for when the flip is likely to happen in the day\n",
    "    subdat_index = ( dat.index.floor('D')>=  index.floor('D')-pd.Timedelta(days=2 -day_offset) )  & ( dat.index.floor('D') <=  index.floor('D')+pd.Timedelta(days=2+day_offset)  )\n",
    "    subdat = dat.iloc[subdat_index]\n",
    "    t = (subdat.index-subdat.index[0]).total_seconds().values\n",
    "    tmax= np.max(t)\n",
    "    t=t/tmax\n",
    "    ax=plt.subplot(2,6,i)\n",
    "    y= subdat.park_pol\n",
    "    plt.plot(abs(subdat.dParker),'.',alpha=.5)\n",
    "    plt.axvline(entry.t_cross,color='k')\n",
    "    if i ==1: plt.ylabel('$\\psi_P$')\n",
    "    ax2=plt.subplot(2,6,i+6)\n",
    "    if i == 1: plt.ylabel('Polarity')\n",
    "    y= subdat.park_pol\n",
    "    ydrop = ( abs(subdat.dParker.values) > 90-rm_perp_limit) & (abs(subdat.dParker.values) < 90 + rm_perp_limit)\n",
    "    plt.plot(subdat.index,y,'.')\n",
    "    plt.plot(subdat.index[~ydrop],y[~ydrop]*.8,'.')\n",
    "    plt.axvline(index+  pd.Timedelta(hours=entry.hrs_offset),color='k')\n",
    "    ax3=ax2.twinx()\n",
    "    plt.plot(subdat.index,expit(entry.coef*t+entry.intercept),color='r')\n",
    "    plt.title(f's = {entry.score:.2f}, rem = {rem_sub.values[ind[i-1]]}')\n",
    "    plt.ylim(0,1)\n",
    "    i+=1\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnetogram and PFSS HCS identification # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:59.294680Z",
     "start_time": "2021-08-04T11:37:59.280298Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class magnetogram:\n",
    "    '''Class to contain magnetograms from GONG\n",
    "    magnetogram.CR = carrington rotation\n",
    "    magnetogram.time = time used to call gong map\n",
    "    magnetogram.gongbr = br data from gong map\n",
    "    magnetogram.gonghead = gong header\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,time,specify=False):\n",
    "        self.time = time\n",
    "        if specify== False:\n",
    "            br,head = DL_GONG(time.to_pydatetime(),datetype='datetime')\n",
    "        if specify:\n",
    "            fname = './GONG/gong'+time[0:4]+time[5:7]+time[8:10]+'.fits'\n",
    "            [[br, head]] = sunpy.io.fits.read(fname)\n",
    "            # this is the radial component of the field. The adjustments are based on pfsspy examples.\n",
    "            br = br - np.nanmean(br) \n",
    "            br = np.roll(br, head['CRVAL1'] + 180, axis=1)\n",
    "        if br.size>1:\n",
    "            self.gongbr,self.gonghead = br,head\n",
    "        else:\n",
    "            self.gonghead = 'No file'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:37:59.337837Z",
     "start_time": "2021-08-04T11:37:59.298153Z"
    }
   },
   "outputs": [],
   "source": [
    "# now I want to produce and save some pfss solutions to my new dataframe. Then I can save them. This takes a long time\n",
    "\n",
    "# make a pfss class that holds the pfss model output, magnetogram time, source surface height\n",
    "class pfss:\n",
    "    '''Class to contain pfss objecs and data\n",
    "    pfss.CR = carrington rotation \n",
    "    pfss.gonghead = gong header file\n",
    "    pfss.input = pfsspy input object\n",
    "    pfss.output = pfsspy output object\n",
    "    pfss.ssbr = source surface magnetic field\n",
    "    pfss.phi = phi grid edges. phi is carrington longitude I think.\n",
    "    pfss.costheta = cos theta grid edges. theta is regular latitude I think\n",
    "    pfss.theta = theta grid edges. theta is  colatitude\n",
    "    pfss.lat = real latitude grid edges\n",
    "    '''\n",
    "    def __init__(self,time,br,header,rss):\n",
    "        # these choices come from the pfsspy examples.\n",
    "        nr = 60\n",
    "        self.gonghead = header \n",
    "        m = sunpy.map.Map(br,header) # make a map from gong data\n",
    "        self.input = pfsspy.Input(m,nr,rss) # set up the pfss input\n",
    "        self.output = pfsspy.pfss(self.input) # this is the code that computes the PFSS solution\n",
    "        self.ssbr = self.output.source_surface_br.data # pull out source surface field 2d array\n",
    "        \n",
    "        # these are the coordinates of the array\n",
    "        self.phi = self.output.grid.pg \n",
    "        self.costheta = self.output.grid.sg\n",
    "        self.theta = np.arccos(self.costheta)\n",
    "        # theta is colatitude\n",
    "        self.lat = -self.theta + np.pi/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T13:22:40.054378Z",
     "start_time": "2021-08-04T13:22:40.043962Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to open saved PFSS files\n",
    "def open_pfss_from_t(time):\n",
    "    datestr = time.translate({ord(i):None for i in ':'})\n",
    "    fname = './PFSS_HDFs/pfss_'+datestr+'.hdf'\n",
    "    DL_Drive.DL_Drive(fname) # download pre-run file from my drive\n",
    "    p = pd.read_hdf(fname,key='2pt5')\n",
    "    if type(p.pfss_2pt5) is np.ndarray: \n",
    "        p = p.pfss_2pt5[1]\n",
    "    elif type(p.pfss_2pt5) is type(None):\n",
    "        p=None\n",
    "    else:\n",
    "        p = p.pfss_2pt5\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T13:15:35.684624Z",
     "start_time": "2021-08-04T13:15:34.900946Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# code for if I'm doing a new PFSS file\n",
    "def solve_new_pfss(df_gong_in,write=True): # df_gong is my magnetogram/pfss dataframe\n",
    "    df_gong=df_gong_in.copy() # this is to avoid overwriting by accident\n",
    "    df_gong['pfss_2pt5'] = '' # pre-define the pfss variable\n",
    "    \n",
    "    # iterate over each entry\n",
    "    for ind in df_gong.index:\n",
    "        entry = df_gong.loc[ind].copy()\n",
    "        mag = magnetogram(entry.gongtime,specify=True) # put the magnetogram into the dataframe. I'm assuming the file already local I think\n",
    "        datestr = entry.gongtime.translate({ord(i):None for i in ':'}) # translate the time into pfss name string\n",
    "        pfssname = './PFSS_HDFs/pfss_'+datestr+'.hdf'\n",
    "        if os.path.isfile(pfssname): # if we have the solution then open it\n",
    "            df_gong.loc[ind,'pfss_2pt5'] =  pd.read_hdf('./PFSS_HDFs/pfss_'+datestr+'.hdf',key='2pt5').values\n",
    "        else: # if we don't then compute it using the class instance\n",
    "            df_gong.loc[ind,'pfss_2pt5'] = pfss(pd.to_datetime(mag.gonghead['DATE']).to_pydatetime(),mag.gongbr,mag.gonghead,2.5)\n",
    "            if write:\n",
    "                df_gong.loc[ind].to_hdf('./PFSS_HDFs/pfss_'+datestr+'.hdf',key='2pt5',mode='w') # and we can write what we computed\n",
    "    return(df_gong)\n",
    "    \n",
    "\n",
    "def read_pfss(df_gong_in): # function to read in pfss stuff and place it into df_gong\n",
    "    df_gong=df_gong_in.copy() \n",
    "    df_gong['pfss_2pt5']=''\n",
    "    for index,row in df_gong.iterrows(): \n",
    "        datestr = row.gongtime.translate({ord(i):None for i in ':'})\n",
    "        df_gong.loc[index] = pd.read_hdf('./PFSS_HDFs/pfss_'+datestr+'.hdf',key='2pt5')\n",
    "    return(df_gong)\n",
    "\n",
    "def read_gong(df_gong_in): # function to read in and store the gong magnetograms\n",
    "    df_gong=df_gong_in.copy()\n",
    "    df_gong['magnetogram'] = ''\n",
    "    df_gong['gongtime'] = ''\n",
    "    # download the gong map\n",
    "    for ind in df_gong.index:\n",
    "        df_gong.loc[ind,'magnetogram'] = magnetogram(ind)\n",
    "        if df_gong.loc[ind,'magnetogram'].gonghead!='No file':\n",
    "            df_gong.loc[ind,'gongtime'] =magnetogram(ind).gonghead['DATE']\n",
    "    df_gong.to_hdf('HCSGONG_MAPs.hdf',key='a',mode='w')\n",
    "    return(df_gong)\n",
    "\n",
    "\n",
    "# this is a bit confusing, but we run these different statements depending on if we already have gong data and/or pfss runs stored\n",
    "# I've not tested recently that these functions still work. But computing the PFSS solutions afresh takes a long time\n",
    "new_gong = False\n",
    "new_pfss = False\n",
    "\n",
    "time_model = sec_df.index\n",
    "\n",
    "if (not new_gong) and (not new_pfss): # this loads in the pre-run stuff.\n",
    "    fname = 'HCSGONG_MAPs.hdf'\n",
    "    DL_Drive.DL_Drive(fname) # download pre-run file from my drive\n",
    "    df_gong_a = pd.read_hdf(fname)\n",
    "    df_gong_a = df_gong_a.drop(columns=['magnetogram'])\n",
    "    df_gong_a = df_gong_a.iloc[df_gong_a.gongtime.values !='']\n",
    "\n",
    "else:\n",
    "    if new_gong and new_pfss:\n",
    "        df_gong_a = pd.DataFrame(index=time_model)\n",
    "        df_gong_a = read_gong(df_gong_a)\n",
    "        df_gong_a = df_gong_a.drop(columns=['magnetogram'])\n",
    "        df_gong_a = df_gong_a.iloc[df_gong_a.gongtime.values !='']\n",
    "        df_gong_a = solve_new_pfss(df_gong_a,write=True)\n",
    "    elif new_pfss and (not new_gong):\n",
    "        fname = 'HCSGONG_MAPs.hdf'\n",
    "        DL_Drive.DL_Drive(fname) # download pre-run file from my drive\n",
    "        df_gong_a = pd.read_hdf(fname)\n",
    "        df_gong_a = df_gong_a.drop(columns=['magnetogram'])\n",
    "        df_gong_a = df_gong_a.iloc[df_gong_a.gongtime.values !='']\n",
    "        df_gong_a = solve_new_pfss(df_gong_a,write=True)\n",
    "    if new_gong: \n",
    "        df_gong_a = pd.DataFrame(index=time_model)\n",
    "        df_gong_a = read_gong(df_gong_a)\n",
    "        df_gong_a = read_pfss(df_gong_a)\n",
    "tlist = df_gong_a.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think next key step is identifying HCS crossing latitudes from the PFSS models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:38:00.331321Z",
     "start_time": "2021-08-04T11:38:00.314952Z"
    }
   },
   "outputs": [],
   "source": [
    "# these are functions to identify the pfss polarity at specified coordinates\n",
    "def gridlookup(xval,yval,xgrid,ygrid,z):\n",
    "    '''define a function to look up direct grid values'''\n",
    "    return( z[ (xval>=xgrid[:-1]) & (xval<xgrid[1:]) , (yval>=ygrid[:-1]) & (yval<(ygrid)[1:]) ] )\n",
    "\n",
    "def br_check(x,y,xgrid,ygrid,z):\n",
    "    br_val = x*np.nan\n",
    "    for i,(xi,yi) in enumerate(zip(x,y)):\n",
    "        br_val[i] = gridlookup(xi,yi,xgrid,ygrid,z)\n",
    "    return(br_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:38:00.513849Z",
     "start_time": "2021-08-04T11:38:00.335288Z"
    }
   },
   "outputs": [],
   "source": [
    "# now I guess I want two dataframes - one for in situ (which I have) and one for pfss+ in situ crossing longitude/time\n",
    "\n",
    "# the in situ crossing time is held in sec_df \n",
    "\n",
    "# just a quick calculation for rectified angle differences \n",
    "def rel_lons(lon,lon0):\n",
    "    return(re_angle_deg(lon0-lon,0,360))\n",
    "\n",
    "# a function to pull out br at the source surface for the CR at the spacecraft's latitude \n",
    "def get_br_df(tlist,df_gong_a,n_lons=180): \n",
    "    lon_ind = np.arange(0,n_lons) \n",
    "\n",
    "    index = pd.MultiIndex.from_product([tlist,lon_ind],names = ['t','lon_ind'] ) # this is a 2d array since we have each intercept and then the range of longitudes for that\n",
    "    dfss =  pd.DataFrame(index=index) # create the blank dataframe\n",
    "\n",
    "    for t,gongt in zip(tlist,df_gong_a.gongtime): # go trough each crossing\n",
    "        timerange = t + pd.to_timedelta(np.linspace(0,27,lon_ind.size),unit='days') \n",
    "        eartheph = ephemeris.get_earth(time=timerange) # pull out the ephemeris for a CR prior to the HCS crossing\n",
    "        lats = eartheph.lat # latitudes from that \n",
    "        lons = sun.L0(time=timerange) # longitudes also \n",
    "        \n",
    "        pfss_ = open_pfss_from_t(gongt) # open the PFSS run\n",
    "        if type(pfss) != type(None): # if it exists...\n",
    "            output = pfss_.output \n",
    "\n",
    "            br = br_check(np.cos(np.pi/2+lats.value*np.pi/180.),lons.value*np.pi/180,pfss_.costheta,pfss_.phi,pfss_.ssbr) # pull out the values along these coordinates\n",
    "\n",
    "            dfss.loc[t,'br'] = br # save them to the dataframe \n",
    "            dfss.loc[t,'lon'] = lons\n",
    "            dfss.loc[t,'lat'] = lats\n",
    "            print(f'Completed t ={gongt}')\n",
    "    return(dfss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T13:17:37.240462Z",
     "start_time": "2021-08-04T13:15:56.717697Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfss is the dataframe of polarities along the source surface\n",
    "\n",
    "new_dfss=False # this takes a while to run so we have a pre-run file to load\n",
    "if new_dfss:\n",
    "    dfss = get_br_df(tlist,df_gong_a,n_lons=180)\n",
    "    dfss['pol'] = np.sign(dfss.br).astype(int)\n",
    "    # write code to save dfss to hdf file:\n",
    "    dfss.to_hdf('dfss_save_180.hdf',key='a',mode='w')\n",
    "else: # even if it exists the rel_lons thing (I guess) takes a while\n",
    "    fname = 'dfss_save_180.hdf'\n",
    "    DL_Drive.DL_Drive(fname) # download pre-run file from my drive\n",
    "    dfss = pd.read_hdf(fname)\n",
    "    for t in tlist:\n",
    "        dfss.loc[t,'rlon'] =rel_lons(dfss.lon,sec_df.lon_cross.loc[t]) # compute the longitude difference from the crossing for each point along the source surface longitude \n",
    "    dfss['rlon']=360-dfss.rlon # flip it to have the correct sign. rlon will be out longitude offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T13:23:50.307706Z",
     "start_time": "2021-08-04T13:23:49.498247Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify, characterise and record in a new DF (maybe) the flips in sign in the pfss runs:\n",
    "\n",
    "dfss['flipsign'] =np.zeros(dfss.index.size)\n",
    "\n",
    "for t in tlist:\n",
    "    subdf = dfss.loc[t]\n",
    "    flipdiff = (subdf.pol - np.roll(subdf.pol,1)).values \n",
    "    flipsign = np.zeros(flipdiff.size) \n",
    "    flipsign[flipdiff>0] = -1\n",
    "    flipsign[flipdiff<0] = 1\n",
    "    dfss.loc[t,'flipsign']=flipsign # so flipsign is != 0 when there is a change in sign ON THAT INDEX compared to the last.\n",
    "    # -1 indicates going from negative to positive (so - )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Combined in situ and PFSS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T13:24:03.356903Z",
     "start_time": "2021-08-04T13:23:52.698449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Alright now we record distance from any flips in the same PFSS model run. Then remove those where it goes back and forth too much\n",
    "# the parameter prox_lim is going to be TRUE when it's too close.\n",
    "\n",
    "dfss['prox_lim'] = False\n",
    "dfss['prox'] = np.nan\n",
    "flip_limit = 30 # this is the minimum distance we allow consecutive flips along the slice\n",
    "\n",
    "for t in tlist: # loop over each crossing\n",
    "    subdf = dfss.loc[t].copy() # the slice in question\n",
    "    check = subdf.flipsign!=0 # identify the entries with a flip in polarity\n",
    "    diffa = re_angle_deg(subdf.rlon[check] - np.roll(subdf.rlon[check],1),-180,180) # the offsets between flips in one direction\n",
    "    diffb = re_angle_deg(subdf.rlon[check] - np.roll(subdf.rlon[check],-1),-180,180) # and the other\n",
    "    drop = np.flatnonzero(check)[(abs(diffa)<flip_limit) | (abs(diffb)<flip_limit)] # drop any where they're too close together\n",
    "    subdf.loc[subdf.index[drop],'prox_lim']=True # flag it if it's dropped\n",
    "    \n",
    "    subdf.loc[subdf.index[check],'prox']= np.nanmin([abs(diffa),abs(diffb)]) \n",
    "    dfss.loc[t,'prox_lim']=subdf.prox_lim.values\n",
    "    dfss.loc[t,'prox']=subdf.prox.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:39:49.253920Z",
     "start_time": "2021-08-04T11:39:48.065962Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTANT!\n",
    "# Now is the code to match up the in situ and source surface crossings\n",
    "\n",
    "for t in tlist: # Loop in situ crossings\n",
    "    subsec = sec_df.loc[t] # the in situ crossing\n",
    "    subss = dfss.loc[t] # the pfss crossings\n",
    "    matchsign = subss.flipsign.values == subsec.fit_flip # identify which flips match in sign\n",
    "    index = matchsign & (~dfss.loc[t,'prox_lim'].values) # only consider those where it matches sign AND they're not too clsoe to others\n",
    "    if np.sum(index)>0:\n",
    "        sec_df.loc[t,'lon_shift'] = np.nanmin( subss.rlon.values[index] ) # record the longitude shift for the nearest one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:39:49.316509Z",
     "start_time": "2021-08-04T11:39:49.256874Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is just the ballistic mapping longitude change\n",
    "def constant_dphi_calc(v,r_in = 1):\n",
    "    r = 1.5e8*r_in\n",
    "    t = r/v\n",
    "    t_day = 1*t/86400\n",
    "    angle = 360*(t_day/25.38)\n",
    "    return(angle)\n",
    "\n",
    "# compute the constant speed mapping for interest\n",
    "sec_df['dphi_con']=constant_dphi_calc(sec_df.V.values)\n",
    "\n",
    "# okay we save this now as an hdf file\n",
    "saver = True\n",
    "fname = 'Matched_crossings_2006_2020.h5'\n",
    "if saver:\n",
    "    sec_df.to_hdf(fname,key='df',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:39:49.765884Z",
     "start_time": "2021-08-04T11:39:49.319486Z"
    }
   },
   "outputs": [],
   "source": [
    "# a check for any trend between the mapping residual and the HCS properties\n",
    "res = sec_df.dphi_con.values-sec_df.lon_shift.values.astype(float)\n",
    "\n",
    "shape = [3,2]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(shape[0],shape[1],1)\n",
    "plt.scatter(sec_df.V,res)\n",
    "plt.xlabel('v')\n",
    "plt.ylabel('res')\n",
    "# plt.ylim(-50,50)\n",
    "plt.subplot(shape[0],shape[1],2)\n",
    "plt.scatter(sec_df.index,res)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('res')\n",
    "plt.subplot(shape[0],shape[1],3)\n",
    "plt.scatter(sec_df.Vgrad,res)\n",
    "plt.xlabel('V gradient')\n",
    "plt.ylabel('res')\n",
    "plt.subplot(shape[0],shape[1],4)\n",
    "plt.scatter(sec_df.score,abs(res))\n",
    "plt.xlabel('Fit score')\n",
    "plt.ylabel('res (abs)')\n",
    "plt.subplot(shape[0],shape[1],5)\n",
    "a=plt.hist(res[sec_df.flip=='+'],bins=20)\n",
    "plt.hist(res[sec_df.flip=='-'],histtype='step',bins=a[1])\n",
    "plt.xlabel('residual')\n",
    "plt.subplot(shape[0],shape[1],6)\n",
    "plt.scatter(sec_df.CarLon,abs(res))\n",
    "# plt.hist(sec_df.CarLon,histtype='step',color='C1')\n",
    "plt.xlabel('Carrington longitude')\n",
    "plt.ylabel('res (abs)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T11:39:49.917660Z",
     "start_time": "2021-08-04T11:39:49.771837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a look at the speed and velocity gradients associated with the crossings\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.hist(sec_df.V.values)\n",
    "plt.subplot(212)\n",
    "plt.hist(sec_df.Vgrad.values,bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-04T13:43:28.998470Z",
     "start_time": "2021-08-04T13:42:53.386807Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1D comparison. This is a paper plot.\n",
    "\n",
    "# Paper uses i = 2 \n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "tlistuse = sec_df.index[np.isfinite(sec_df.lon_shift)]\n",
    "i = choice(np.arange(0,tlistuse.size))\n",
    "i=2\n",
    "t = tlistuse[i]\n",
    "\n",
    "dat_ind = abs(t - dat.index) < pd.Timedelta('14D')\n",
    "\n",
    "fig = plt.figure(figsize=(5,7),constrained_layout=2)\n",
    "gs = gridspec.GridSpec(ncols=1, nrows=7, figure=fig)\n",
    "\n",
    "p=dfss.loc[t].pol.values\n",
    "\n",
    "ax1=plt.subplot(gs[0:2, 0])\n",
    "plt.title(f'Unshifted. t = {t}')\n",
    "plt.plot(dfss.loc[t].lon,dfss.loc[t].pol,'.')\n",
    "plt.plot(dfss.loc[t].lon[p>0],dfss.loc[t].pol[p>0],'.',color='C3')\n",
    "plt.plot(dat.CarLon[dat_ind],dat.park_pol[dat_ind]*.8,'.',color='k',alpha=.8)\n",
    "plt.axvline(sec_df.loc[t].lon_cross,color='k',alpha=.8)\n",
    "plt.plot(dfss.loc[t].lon[dfss.loc[t].flipsign!=0],dfss.loc[t].flipsign[dfss.loc[t].flipsign != 0]*1.2,'.',color='C2')\n",
    "\n",
    "\n",
    "plt.yticks([-1,0,1])\n",
    "plt.xlim(360,0)\n",
    "plt.ylabel('Polarity')\n",
    "\n",
    "plt.subplot(gs[2:4, 0])\n",
    "plt.title(f'Shifted by {sec_df.loc[t].lon_shift:.1f}'+'$^{\\circ}$')\n",
    "plt.plot(dfss.loc[t].lon,dfss.loc[t].pol,'.')\n",
    "plt.plot(dfss.loc[t].lon[p>0],dfss.loc[t].pol[p>0],'.',color='C3')\n",
    "plt.plot(re_angle_deg(dat.CarLon[dat_ind].values+sec_df.loc[t].lon_shift,0,360),dat.park_pol[dat_ind]*.8,'.',color='k',alpha=.8)\n",
    "plt.axvline(sec_df.loc[t].lon_cross+sec_df.loc[t].lon_shift,color='k',alpha=.8)\n",
    "# plt.axvline(sec_df.loc[t].lon_cross+sec_df.loc[t].dphi_con,color='C3')\n",
    "plt.plot(dfss.loc[t].lon[dfss.loc[t].flipsign!=0],dfss.loc[t].flipsign[dfss.loc[t].flipsign != 0]*1.2,'.',color='C2')\n",
    "plt.xlim(360,0)\n",
    "plt.ylabel('Polarity')\n",
    "plt.yticks([-1,0,1])\n",
    "\n",
    "# plot on here the model data too\n",
    "plt.subplot(gs[4:7, 0],sharex=ax1)\n",
    "timerange = time_model[i] + pd.to_timedelta(np.linspace(0,27.3,360),unit='days')\n",
    "eartheph = ephemeris.get_earth(time=timerange)\n",
    "lats = eartheph.lat\n",
    "lons=sun.L0(time=timerange)\n",
    "\n",
    "# now produce a polarity profile along these longitudes and plot on top to verify\n",
    "t =df_gong_a.loc[t].gongtime\n",
    "\n",
    "pfss_ = open_pfss_from_t(t)\n",
    "\n",
    "output = pfss_.output\n",
    "\n",
    "br = br_check(np.cos(np.pi/2+lats.value*np.pi/180.),lons.value*np.pi/180,pfss_.costheta,pfss_.phi,pfss_.ssbr)\n",
    "\n",
    "costhetagrid,longrid=np.meshgrid(pfss_.costheta,pfss_.phi)\n",
    "mesh = plt.pcolormesh(longrid*180./np.pi,costhetagrid,np.sign(pfss_.ssbr.T),cmap=plt.get_cmap('bwr'),vmin=-4,vmax=4)\n",
    "# plt.colorbar()\n",
    "plt.scatter(lons.value[br>0],np.cos(np.pi/2+lats.value*np.pi/180.)[br>0]*0-.1,color='C3',s=6)\n",
    "plt.scatter(lons.value[br<0],np.cos(np.pi/2+lats.value*np.pi/180.)[br<0]*0-.1,color='C0',s=6)\n",
    "plt.xlabel('$\\phi$ ($^{\\circ}$)')\n",
    "plt.title('PFSS Model')\n",
    "plt.ylabel(r'cos($\\theta$)')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('pfss_insitu_match_example.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
